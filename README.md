# Efficient-AI-Computing-Resources

## Transformer

**Efficient transformers: A survey.**<br>
*Y Tay, M Dehghani, D Bahri, D Metzler.*<br>
ACM Computing Surveys, 2022.
[[Paper](https://dl.acm.org/doi/pdf/10.1145/3530811)]

**A survey on efficient training of transformers.**<br>
*B Zhuang, J Liu, Z Pan, H He, Y Weng, C Shen.*<br>
arXiv:2302.01107, 2023.
[[Paper](https://arxiv.org/pdf/2302.01107)]

**Full stack optimization of transformer inference: a survey.**<br>
*S Kim, C Hooper, T Wattanawong, M Kang, R Yan, H Genc, G Dinh, Q Huang, K Keutzer, et al.*<br>
arXiv:2302.14017, 2023.
[[Paper](https://arxiv.org/pdf/2302.14017)]

## LLM

**AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration.**<br>
*Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, Song Han.*<br>
MLSys, 2024.
[[Paper](https://arxiv.org/pdf/2306.00978)]
[[Github](https://github.com/mit-han-lab/llm-awq)]

**QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving.**<br>
*Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, Song Han.*<br>
ArXiv, 2024.
[[Paper](https://arxiv.org/pdf/2405.04532)]
[[Github](https://hanlab.mit.edu/projects/qserve)]

**Efficient streaming language models with attention sinks.**<br>
*G Xiao, Y Tian, B Chen, S Han, M Lewis.*<br>
ICLR, 2024.
[[Paper](https://arxiv.org/pdf/2309.17453)]
[[Github](https://github.com/mit-han-lab/streaming-llm)]

**Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference.**<br>
W Luk, KFC Yiu, R Li, K Mishchenko, SI Venieris, H Fan.*<br>
arxiv:2405.18628, 2024.
[[Paper](https://arxiv.org/pdf/2405.18628)]
[[Github](https://github.com/hmarkc/parallel-prompt-decoding)]

## VLP

**Efficientvlm: Fast and accurate vision-language models via knowledge distillation and modal-adaptive pruning.**<br>
*T Wang, W Zhou, Y Zeng, X Zhang.*<br>
arxiv:2210.07795, 2022.
[[Paper](https://arxiv.org/pdf/2210.07795)]
[[Github](https://github.com/swaggy-TN/EfficientVLM)]

**MobileVLM: A Fast, Strong and Open Vision Language Assistant for Mobile Devices.**<br>
*Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, et al.*<br>
arXiv, 2023.
[[Paper](https://arxiv.org/pdf/2312.16886v2)]
[[Github](https://github.com/Meituan-AutoML/MobileVLM)]

**MobileVLM V2: Faster and Stronger Baseline for Vision Language Model.**<br>
*X Chu, L Qiao, X Zhang, S Xu, F Wei, Y Yang, et al.*<br>
ArXiv, 2024.
[[Paper](https://arxiv.org/pdf/2402.03766)]
[[Github](https://github.com/Meituan-AutoML/MobileVLM)]

## Diffusion Models

**Snapfusion: Text-to-image diffusion model on mobile devices within two seconds.**<br>
*Y Li, H Wang, Q Jin, J Hu, P Chemerys, Y Fu, Y Wang, S Tulyakov, J Ren.*<br>
Advances in Neural Information Processing Systems, 2024.
[[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/41bcc9d3bddd9c90e1f44b29e26d97ff-Paper-Conference.pdf)]
[[Github](https://snap-research.github.io/SnapFusion)]

**DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models.**<br>
*Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, Song Han.*<br>
CVPR, 2024.
[[Paper](https://arxiv.org/pdf/2402.19481)]
[[Github](https://github.com/mit-han-lab/distrifuser)]

**Deepcache: Accelerating diffusion models for free.**<br>
*X Ma, G Fang, X Wang, et al.*<br>
CVPR, 2024.
[[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_DeepCache_Accelerating_Diffusion_Models_for_Free_CVPR_2024_paper.pdf)]
[[Github](https://github.com/horseee/DeepCache)]
